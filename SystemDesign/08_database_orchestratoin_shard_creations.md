# Data Base Orchestration and Shard Creation

### ğŸ”¹ **1. What â€œOrchestrationâ€ Means**

In general, **orchestration** means automating and managing multiple complex operations so that they work together smoothly â€” like conducting an orchestra.

In databases, this means managing **many database instances, replicas, and services** automatically, ensuring they:

* Run in sync
* Stay healthy
* Scale as needed
* Recover from failures
* Handle schema or version changes with minimal downtime

---

### ğŸ”¹ **2. Key Components of Database Orchestration**

| Component                    | Description                                                                        |
| ---------------------------- | ---------------------------------------------------------------------------------- |
| **Provisioning**             | Automatically creating and initializing new database instances.                    |
| **Configuration Management** | Applying consistent configuration (e.g., user roles, ports, replication settings). |
| **Scaling**                  | Increasing or decreasing database nodes depending on load.                         |
| **Replication & Sharding**   | Managing replicas and splitting data for horizontal scaling.                       |
| **Failover Handling**        | Automatically promoting replicas to primary if a failure occurs.                   |
| **Backups & Recovery**       | Scheduling backups and restoring them when needed.                                 |
| **Monitoring & Alerting**    | Tracking performance, latency, and resource usage.                                 |

---

### ğŸ”¹ **3. Examples of Database Orchestration Tools**

| Tool                           | Description                                                                                        |
| ------------------------------ | -------------------------------------------------------------------------------------------------- |
| **Kubernetes Operators**       | Automate DB lifecycle in Kubernetes (e.g., MySQL Operator, MongoDB Operator, PostgreSQL Operator). |
| **Vitess**                     | A database clustering system for scaling MySQL horizontally (used by YouTube).                     |
| **Crunchy Data Operator**      | PostgreSQL operator for automated deployment, scaling, and backup.                                 |
| **AWS RDS / Aurora**           | Cloud-managed services that perform orchestration under the hood.                                  |
| **Google Cloud SQL / AlloyDB** | Cloud-managed databases with orchestration features.                                               |

---

### ğŸ”¹ **4. Why It Matters**

Database orchestration helps achieve:

* âœ… **High availability**
* âš¡ **Auto-scaling**
* ğŸ”„ **Self-healing clusters**
* ğŸ§© **Consistent deployments**
* ğŸ“¦ **Infrastructure as Code (IaC) integration**

Itâ€™s especially important in **microservices** and **cloud-native** systems, where multiple apps and databases need to coordinate efficiently.

---

### ğŸ”¹ **5. Example Scenario**

Imagine youâ€™re running a PostgreSQL cluster in Kubernetes:

* A **PostgreSQL Operator** automatically deploys 1 primary and 2 replicas.
* If the primary fails, the operator promotes a replica automatically.
* Backups are scheduled daily.
* Scaling rules add more replicas during peak load.
  ğŸ‘‰ All of this happens automatically â€” thatâ€™s **database orchestration**.

---

# ğŸ§­ Consistent Hashing and Database Orchestration

---

## âš™ï¸ Consistent Hashing (under different settings)

The routing algorithm (**consistent hashing**) determines the **sharding**.
Data is created on the go â€” if all of Swetha's requests go to **Server C**, then her data will automatically be in **Server C**.

---

### ğŸ”¹ Sharding (without Replication)

Each server gets **different data** â€“ thereâ€™s **no overlap** of data across different servers.

In general:

| Term       | Meaning                                                       |
| ---------- | ------------------------------------------------------------- |
| **Shard**  | A piece or partition of the overall data.                     |
| **Server** | A physical or virtual machine that stores one or more shards. |

* Sometimes, **1 server contains multiple shards**.
* Sometimes, **1 server = 1 shard**.
* But **a single shard is never split** across multiple servers (it can only be *replicated*).

---

### â“ Shard vs Server (in the case of Sharding without Replication)

* Every server contains a **different shard**.
* Therefore, **1 Server = 1 Shard**.
* Each server has a **unique set of data** (no duplication).

---

### â“ What gets placed on the ring?

* **Servers (== shards)** get placed on the **consistent hashing ring**.

---

### â“ Can a "shard" go down?

Yes.
A shard (which equals one server in this setting) can go down.
In that case, the users who were originally routed to that shard will now be **routed automatically to the next server on the ring**.

However â€” note that while routing may continue, the **data that was stored on the failed shard** is **lost** (since thereâ€™s no replication).

---

### â“ How does the data reach the next server in the ring?

When a server fails:

* The consistent hashing algorithm **re-maps the key range** that the failed shard was responsible for.
* The **next server clockwise on the ring** temporarily becomes responsible for that key range.
* **New data** for those users will now be written to this new server.
* **Old data** (if any) may be lost, since thereâ€™s no replication in this model.

---

## âš™ï¸ Sharding + Master-Slave Replication

### â“ Shard vs Server

* **1 Shard = X Servers**, where **X = replication factor**.
* Each shard is made up of **multiple servers** in a **master-slave** (or leader-follower) configuration.
* All servers in the same shard contain **the same data** (due to replication).

---

### â“ What gets placed on the ring?

* **Shards** (not individual servers) are placed on the **consistent hashing ring**.
* Each shard is a *logical unit* consisting of one master and multiple replicas.

---

### â“ Can a "shard" go down?

* **No**, an entire shard going down is **highly unlikely**.
* Because a shard consists of multiple servers â€” for a shard to fail, **all servers** in that shard must fail simultaneously.

---

### â“ What happens when a server crashes?

**No worries!**
The shard as a whole continues to function.

* If the **master** within a shard crashes â†’ the **slaves elect a new master**.
* If a **slave** crashes â†’ other slaves still hold replicas, so reads can continue.
* The system remains **available and consistent** (depending on replication policy).

---

## ğŸ§© Database Orchestration

Database orchestration is the process of **coordinating multiple database servers** (each with shards and replicas) to behave like **a single, self-managing system**.

It handles:

* Configuration management
* Replication
* Health checks
* Failover
* Recovery
* Scaling

---

### ğŸ”¹ Replication Factor (X)

The **replication factor (RF)** is the **minimum number of copies** maintained for any piece of data.

Example:
If **RF = 4**, then every shardâ€™s data is replicated across **4 servers** (1 master + 3 slaves).

This ensures:

* **Fault tolerance**
* **High availability**
* **Data durability**

---

### ğŸ”¹ Configuration Management

When multiple servers work together as a distributed database, they must **share a common configuration** to remain in sync.

This configuration includes:

| Configuration Element   | Description                                               |
| ----------------------- | --------------------------------------------------------- |
| **Server Availability** | Which servers are alive (via heartbeat or health checks). |
| **Shard Mapping**       | What shards exist, and which servers host each shard.     |
| **Replication Roles**   | Who is the master, who are the slaves.                    |
| **Replication Factor**  | How many replicas exist per shard.                        |
| **System Settings**     | SQL dialect, credentials, connection info, etc.           |

---

### ğŸ”¹ Distributed Configuration Management System

To ensure **consistency across servers**, a **distributed configuration management system** is used.
This ensures all nodes always **agree** on the state of the system.

#### Responsibilities:

1. Maintain the current configuration (server list, shard map, replication roles).
2. Detect failures and trigger recovery mechanisms (e.g., elect new master).
3. Propagate updates atomically across all nodes.
4. Enable coordination between shards.

#### Common Tools:

* ğŸ˜ **Apache ZooKeeper**
* ğŸ§© **etcd**
* ğŸ§­ **Consul**

---

### ğŸ”¹ Why ZooKeeper?

Apache ZooKeeper is a **centralized service for distributed coordination**.

In the context of databases:

* It stores **cluster metadata**.
* Maintains **heartbeats** to detect server failures.
* Manages **leader election**.
* Helps clients discover which server (or shard) to contact.

---

### ğŸ”¹ How It All Fits Together

Hereâ€™s what happens in a **sharded + replicated + orchestrated** database setup:

1. **Consistent Hashing** decides **which shard** stores each data key.
2. Each **shard** has **X servers** (replicas).
3. A **master** handles writes; **slaves** replicate from it.
4. **ZooKeeper (or etcd)** maintains cluster configuration:

   * Which shards exist
   * Which servers are in each shard
   * Whoâ€™s the current master
5. If a master dies â†’ **ZooKeeper triggers leader election**.
6. If a new server is added â†’ **ZooKeeper updates mappings** and rebalances shards.

---

### ğŸ”¹ Real-World Examples

| System               | Coordination Tool | Notes                                          |
| -------------------- | ----------------- | ---------------------------------------------- |
| **HBase**            | Apache ZooKeeper  | Tracks region servers and masters.             |
| **Kafka**            | ZooKeeper / KRaft | Tracks brokers, topics, and leader partitions. |
| **Cassandra**        | Gossip protocol   | Peer-to-peer coordination without ZooKeeper.   |
| **Vitess (YouTube)** | etcd              | Orchestrates MySQL clusters at scale.          |
| **MongoDB**          | Config servers    | Manage shard metadata and replica sets.        |

---

### ğŸ§© Visualization

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Distributed Config Store (ZooKeeper)     â”‚
        â”‚ Maintains shard map, roles, health, etc.   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                      â”‚                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard A   â”‚        â”‚  Shard B   â”‚        â”‚  Shard C   â”‚
â”‚ (1 master  â”‚        â”‚ (1 master  â”‚        â”‚ (1 master  â”‚
â”‚ + replicas)â”‚        â”‚ + replicas)â”‚        â”‚ + replicas)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

All shards work independently, but ZooKeeper (or etcd) ensures they stay in sync and self-heal when something fails.

---

### ğŸ”¹ Benefits of Database Orchestration

âœ… **High Availability** â€“ Automatic failover via replication
âš¡ **Scalability** â€“ New shards/servers can join dynamically
ğŸ§  **Consistency** â€“ Shared configuration across the cluster
ğŸ” **Self-Healing** â€“ Auto recovery from crashes
ğŸ“¦ **Automation** â€“ Less manual database management

---

# ğŸ§  Database Orchestration (Detailed View)

---

## âš™ï¸ Orchestrator â€” *The Brain of the Distributed Database*

The **Orchestrator** is the **control plane** or **brain** behind a distributed database system.
It continuously monitors the state of the cluster â€” all shards, servers, replication factors, and load distribution â€” and takes decisions to keep the system **balanced, consistent, and fault-tolerant**.

Think of it as the **air traffic controller** for your database cluster.

---

### ğŸ§© The Orchestrator is responsible for:

#### 1. Maintaining the **Replication Factor**

* Each shard must always have *X copies* of its data (as per the replication factor).
* If a server within a shard crashes, the number of available replicas drops.
* The Orchestrator must **add a new server** to that shard to restore the replication factor.

> ğŸ§  Example:
> If RF = 4 and one slave in a shard fails â†’ replication factor temporarily becomes 3.
> The Orchestrator immediately assigns a *spare server* to that shard and triggers **data replication** from the master until the new server catches up.

---

#### 2. Deciding **Server-to-Shard Assignments**

* The Orchestrator maintains a **mapping** of which servers belong to which shard.
* When new servers are added, it determines whether to:

  * Attach them to **existing shards**, or
  * Create **new shards** (for scaling out).

---

#### 3. **Scaling Out / Scaling In**

* **Scale Out (Add Shards):**
  When system utilization is high (e.g., >90%), the Orchestrator adds *new shards* to reduce load.
* **Scale In (Remove Shards):**
  When traffic decreases or resources must be reclaimed, it can merge or remove shards safely.

---

#### 4. **Load Balancing**

* Ensures that load (both read and write) is distributed **evenly** across shards and servers.
* Works in coordination with the **Load Balancer (LB)**, which handles real-time routing.
* If certain shards are overloaded, the Orchestrator can **rearrange data distribution** or move users/data ranges to new shards.

---

## âš™ï¸ Adding New Servers

Letâ€™s break this down as a sequence of events:

---

### ğŸ§© Step 1: Detect Need for Expansion

Imagine the distributed database is running at **90% capacity** â€” nearing its limits.

Either:

* A **human operator**, or
* An **auto-scaling service**

decides to **add more servers** to handle increasing traffic.

---

### ğŸ§© Step 2: Provision New Servers

These new servers are provisioned with the following setup:

1. **Provision hardware / VMs**
2. **Install database software**
3. **Configure database networking, URLs, ports, credentials**
4. **Register servers** with the **Orchestrator**

Once registered, these servers become **visible** to the Orchestrator as available capacity.

---

### ğŸ§© Step 3: Orchestratorâ€™s Decision â€” What to Do With the New Servers?

#### â“ Should we assign these servers to an existing shard?

**No.**
Each existing shard already has the desired **replication factor (X)**.

If we add more servers to an existing shard, it would **increase the replication factor unnecessarily**, leading to resource waste and imbalance.

Instead, we use these servers to **create new shards**, allowing **horizontal scaling**.

---

### ğŸ§© Step 4: Create New Shards

If replication factor = X and number of new servers = N,
then we can create up to:

[
\text{New Shards} = \text{floor}(N / X)
]

> Example:
> If RF = 4 and 8 new servers are added â†’ we can create `floor(8 / 4) = 2` new shards.

Each new shard will have:

* 1 master
* (X â€“ 1) slave replicas

---

### ğŸ§© Step 5: Keep Some Servers in Reserve

Itâ€™s important **not** to use *all* new servers immediately.

Why?

* If all servers are used for new shards, and later one server crashes,
  there will be **no free servers** available to maintain the replication factor.
* Reserved (idle) servers act as **standby nodes** for **failover recovery**.

---

### ğŸ§© Step 6: Distributing Data to New Shards

When new shards are added, we must **rebalance data** from existing shards.

This must happen **seamlessly**, with **minimal downtime**.

#### Process:

1. **Simulation Phase**

   * The Orchestrator simulates how data redistribution will occur.
   * It calculates the data ranges (e.g., consistent hash key ranges) to migrate.
2. **Real Phase**

   * Actual data movement begins.
   * Background replication ensures that users experience **zero downtime**.
   * Once migration completes, routing tables are updated to direct future requests to new shards.

---

## ğŸ’¥ Failure Recovery

### â“ When a Server Crashes Within a Shard

* The Orchestrator detects failure (via heartbeat timeout).
* It immediately **assigns a reserved (standby) server** to the affected shard.
* The new server:

  * Has no data initially.
  * Syncs up with the shardâ€™s master to **replicate all data**.
  * Once fully caught up, it joins the shard as a functional slave.

---

### â“ When a Crashed Server Recovers

When a previously crashed server comes back online:

* It is **treated as a new server**.
* Any old data is **wiped out** to avoid inconsistencies.
* It is moved into the **reserved pool**, ready for future assignments.

---

## ğŸ§© How All This Works Together

| Step | Component                                    | Function                                                             |
| ---- | -------------------------------------------- | -------------------------------------------------------------------- |
| 1    | **Auto-Scaling Service**                     | Adds or removes physical servers.                                    |
| 2    | **Orchestrator**                             | Assigns servers to shards, maintains replication, manages balancing. |
| 3    | **Load Balancer (LB)**                       | Routes requests to the right shard/server.                           |
| 4    | **Configuration Manager (ZooKeeper / etcd)** | Keeps metadata and shard-server mapping consistent.                  |

---

### ğŸ§­ Example Real-World Analogues

| System        | Orchestrator Role                                                       | Configuration Management |
| ------------- | ----------------------------------------------------------------------- | ------------------------ |
| **Vitess**    | Handles shard splitting, rebalancing, and topology management for MySQL | Uses **etcd**            |
| **Cassandra** | Internal orchestration via gossip protocol                              | No central orchestrator  |
| **MongoDB**   | Config servers + mongos router manage shards and replication            | Internal config service  |
| **HBase**     | HMaster acts as orchestrator                                            | Uses **ZooKeeper**       |

---

### ğŸ§© Visualization

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 ORCHESTRATOR                 â”‚
        â”‚----------------------------------------------â”‚
        â”‚ 1. Maintain replication factor               â”‚
        â”‚ 2. Assign servers to shards                  â”‚
        â”‚ 3. Add/remove shards (scaling)               â”‚
        â”‚ 4. Balance load across cluster               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚              â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Existing Shards  â”‚                 â”‚    New Shards      â”‚
â”‚  (Master + Slaves) â”‚                 â”‚ (Created via new   â”‚
â”‚                    â”‚                 â”‚   servers)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### âš¡ Key Takeaways

| Concept                | Summary                                                                   |
| ---------------------- | ------------------------------------------------------------------------- |
| **Orchestrator**       | The brain that maintains replication, shard assignment, and load balance. |
| **Replication Factor** | Ensures each shard has enough data copies.                                |
| **Scaling Out/In**     | Adds or removes shards based on cluster capacity.                         |
| **Reserved Servers**   | Standby nodes for handling failures.                                      |
| **Recovery**           | Crashed servers are replaced and re-synced automatically.                 |

---


Perfect ğŸ‘ â€” this â€œReserved Serversâ€ section fits beautifully as the next module after **Orchestrator** in a distributed database architecture document.

Hereâ€™s your content slightly refined for **clarity, flow, and completeness**, while keeping *all* your points intact and adding just a few enhancements for depth and readability:

---

## ğŸ§  Reserved Servers

Even the most well-designed systems face failures â€” hardware crashes, network outages, or maintenance downtimes. To handle such failures gracefully without downtime or data loss, **the orchestrator maintains a pool of reserved servers**.

Reserved servers act as **standby nodes** â€” always ready to be swapped in when any active database server fails within a shard.

---

### ğŸ’¡ Why Reserved Servers?

If any server within a shard crashes, the **replication factor** (number of copies per shard) temporarily drops. To restore it immediately, the orchestrator replaces the failed server with one of the reserved servers.

Thus, having reserved servers ensures:

* No service interruption during server failure.
* Immediate restoration of replication factor.
* High availability and resilience of the database cluster.

---

### ğŸ“Š Estimating the Number of Reserved Servers

The **optimal number** of reserved servers depends on:

1. **Total number of servers** (N) in the system.
2. **Failure probability** of each server [P(Crash) : Probability of Crash].

If we know:

* A typical server fails once per year.
* Recovery/repair takes **2 days**.

Then:

```
P(Crash) = 2/365 = aprox 0.5%

```

Expected number of crashed servers on any given day [E(Down Server) : Expected no of servers can down]:

```
E(Down Server) = N * P(Crash)
```

Example:

* N = 1000 servers
* E = 1000 Ã— 0.005 = **5 servers (expected)**

So on average, ~5 servers will be down on any given day.

---

### âš ï¸ But Expectation Isnâ€™t Enough!

If we maintain exactly 5 reserved servers:

* Half the days, thatâ€™s enough.
* But half the days, more than 5 servers might be down â†’ **insufficient reserves**.

Hence, the orchestrator should maintain **enough reserved servers to cover failures with 99% probability**:

```
P(numberÂ ofÂ crashedÂ serversÂ â‰¤Â R) > 99%
```

Where **R** = minimum number of reserved servers to maintain.

This ensures the system almost never runs out of spare servers, even under rare failure spikes.

---

### âš ï¸ Why â€œExpectation Isnâ€™t Enoughâ€

---

#### ğŸ§® 1. What does â€œExpectationâ€ mean?

When we say the *expected number* of crashed servers is **5**, it means:

> â€œOn average, we expect around 5 servers to be down on any given day.â€

Itâ€™s a **statistical average**, not a guarantee.

Some days:

* Maybe only **2** servers crash.
  Other days:
* Maybe **8 or 10** servers crash.

The *expectation* (mean) just tells you what happens **on average**, over a long period â€” not what happens **every day**.

---

#### ğŸ¯ 2. Why thatâ€™s not enough for reliability

If we only keep **5 reserved servers**, weâ€™re planning for the *average* case.
But distributed systems need to survive **worst-case** days too â€” when more servers crash than expected.

Think of it like this:

| Day | Servers Crashed | Reserved Servers Available        | Result         |
| --- | --------------- | --------------------------------- | -------------- |
| Mon | 3               | âœ… 5 available                     | OK             |
| Tue | 5               | âœ… 5 available                     | OK             |
| Wed | 8               | âŒ Only 5 reserved â†’ insufficient! | Failures start |

So even though **5 is the expected number**, itâ€™s **not safe enough** â€” because 8 crashes in one day isnâ€™t impossible.
And when it happens, your system wonâ€™t have enough backup servers to maintain replication â€” leading to reduced availability or even data risk.

---

#### ğŸ“Š 3. How to make it reliable

To make sure your system almost *always* has enough reserves, we plan for the **99th percentile**, not just the mean.

That means we choose **R (reserved servers)** such that:

> Thereâ€™s only a 1% chance that more than R servers crash in a day.

```
P(numberÂ ofÂ crashedÂ serversÂ â‰¤Â R) > 99%
```

So even on bad days, weâ€™re still covered.

---


#### To ensure 99% reliability, if each server has a 0.5% daily crash chance and you have 1000 servers (expected 5 crashes/day), you should keep about 10â€“11 reserved servers instead of just 5.


### Step-by-step reasoning

#### 1ï¸âƒ£ Given

* Total servers (N = 1000)
* Probability a server crashes on any day (p = 0.005)
* Expected crashes per day E[X] = N Ã— p = 5

---

#### 2ï¸âƒ£ Model

Each server can either **crash (1)** or **not crash (0)** independently.
So total crashes per day follow a **Binomial distribution**:

```
Xâˆ¼Binomial(N=1000,p=0.005)
```

For large N, small p, we can approximate this by a **Poisson(Î» = 5)** distribution.

---

#### 3ï¸âƒ£ Find R for 99% reliability

We want the smallest (R) such that:

```
P(Xâ‰¤R) â‰¥ 0.9
```

Using the Poisson cumulative distribution with Î» = 5:

| R      | P(X â‰¤ R)    |
| ------ | ----------- |
| 5      | 0.616       |
| 6      | 0.762       |
| 7      | 0.866       |
| 8      | 0.938       |
| 9      | 0.972       |
| **10** | **0.989** âœ… |
| 11     | 0.995 âœ…     |

---

#### âœ… Therefore

To cover **99% of days**, you need **R â‰ˆ 10 or 11 reserved servers**,
even though the *average* expected crashes per day is only 5.

---

### ğŸ’¤ Should Reserved Servers Stay Idle?

**No â€” servers are expensive!**

We want every server to be useful, even if itâ€™s reserved. So, we keep them **active as extra read replicas**.

#### âœ… Reserved Server Utilization Strategy:

1. Reserved servers are added as **read replicas** in existing shards.
2. They are marked as **â€œreservedâ€** â€” available for replacement if any server fails.
3. They **do not become master nodes**.
4. They can handle part of the read traffic â€” improving read scalability.

---

### ğŸ“˜ Important Notes

* The **replication factor (X)** is a *minimum requirement*, not a hard cap.
  You can have more replicas than X, but never fewer.

* Reserved servers acting as read replicas *exceed* the replication factor temporarily, which is fine as long as resources arenâ€™t wasted.

---

### ğŸ” Failure & Recovery Workflow

**Case 1: Server crash within a shard**

* If the shard has extra read replicas (reserved servers):
  â†’ Mark one as *active* (no longer reserved).
* Otherwise:
  â†’ The orchestrator reassigns a reserved server from another shard.
  â†’ Wipes its data and syncs it with the crashed shardâ€™s data.

**Case 2: Crashed server recovers**

* The recovered server is **re-initialized** and moved to the **reserved pool**.
* Any old data on it is discarded to maintain consistency.

Throughout all this, **no downtime** occurs for users â€” other replicas in the shard continue handling reads/writes.

---

### ğŸ§© Summary

| Concept              | Purpose                                          |
| -------------------- | ------------------------------------------------ |
| **Reserved Servers** | Standby servers for handling crashes             |
| **Optimal Count**    | Determined statistically using crash probability |
| **Utilization**      | Act as read replicas when idle                   |
| **Failover Action**  | Immediately replace crashed nodes                |
| **Recovery**         | Repaired nodes rejoin reserved pool              |

---

Perfect! We can now enhance your content by keeping **all your original points**, adding clarity, additional notes, and a **visual diagram** to make seamless shard creation easier to understand. Hereâ€™s the enriched version:

---

# Seamless Shard Creation

When the orchestrator decides to create a new shard from new servers, the **data movement** happens as follows:

1. The new shard must take some data from **every existing shard**.

   * Reduce the load of each existing shard **equally**.
2. There should be **minimal downtime** for the users being migrated to this new shard.

---

### âš ï¸ Why the simplest approach doesnâ€™t work (single-phase operation)

1. Orchestrator creates a new shard.
2. Initially, this shard is **empty**.

   * Data copy starts (can take hours, e.g., 2 hours).
3. LB immediately starts sending users to the new shard.
4. Problem: the shard **doesnâ€™t have data yet** â†’ users may see missing data.

**Naive solutions:**

* Wait until all data is copied (2-hour downtime) â†’ high disruption.
* Allow users to access the shard (eventual consistency) â†’ data temporarily inconsistent, bad user experience.

**Conclusion:** Single-phase approach is unacceptable; we need **two-step migration**.

---

## 1ï¸âƒ£ Simulation Phase

Orchestrator creates the shard **silently**:

1. **Simulation without LB awareness:**

   * Knows all metadata via config management:

     * Users, servers, consistent-hash info.
   * Determines **which users** will move to the new shard.
2. **Bulk copy of historical data**:

   * For each source shard:

     ```sql
     SELECT * FROM table1, table2, ...
     WHERE user_id IN (4, 9, 12, ...)
     ```

     â†’ Insert into new shard.
   * Letâ€™s assume this runs from `t0 â†’ t1` (2 hours).

**Delta problem:**

* New data created during bulk copy is **not captured** (cursor isolation).
* `Delta` = new writes during t0 â†’ t1.
* Usually small (e.g., 2 hours of transactions â†’ 30 seconds copy).

---

### Visualization â€” Simulation Phase

```
t0: start bulk copy
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Old Shards    â”‚ <--- LB continues routing requests here
 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ bulk copy selected users â†’ New Shard
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ New Shard     â”‚  empty initially, receiving bulk data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
t0 â†’ t1: delta writes captured in background
```

---

## 2ï¸âƒ£ Real Phase

Starts at `t1` after bulk copy:

1. Orchestrator informs LB about new shard.
2. LB begins routing requests to the new shard.
3. **Delta copy** (WAL/binlog based):

   ```sql
   SELECT * FROM table1, table2, ...
   WHERE user_id IN (4, 9, 12, ...)
   AND modified_at >= t0
   ```

   * Applies missing transactions to the new shard.
4. Delta typically small â†’ fast copy (seconds).

---

### Consistency Options

| Option                                  | Pros                  | Cons                                                          |
| --------------------------------------- | --------------------- | ------------------------------------------------------------- |
| Serve requests while delta copies       | No downtime           | Users may see missing data temporarily (eventual consistency) |
| Deny requests until delta copy finishes | Immediate consistency | Minimal downtime (seconds)                                    |

> Goal: reduce downtime from **2 hours â†’ few seconds** for immediate consistency.

---

### Why two-step works

1. **Single-step**:

   * Copying 1 TB â†’ wait for 1â€“2 hours for consistency â†’ long downtime.
2. **Two-step**:

   * Bulk copy large historical data **without affecting users**.
   * Delta (small) copied while LB routes traffic.
   * Immediate consistency downtime reduced from **2 hours â†’ a few seconds**.

---

### Visualization â€” Real Phase

```
t1: bulk copy complete, delta remaining
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ New Shard     â”‚  delta copy starts
 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ LB starts routing requests here
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Users         â”‚  Some requests may wait (if enforcing consistency)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
t2: delta copy complete
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ New Shard     â”‚  fully consistent, serving all requests
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

Hereâ€™s a polished, enriched version of your **Sharding + Multi-Master Replication** notes, keeping all your points and adding clarity, structure, and some visualization ideas:

---

# Sharding + Multi-Master Replication

Multi-master replication **removes the strict master-slave distinction**.

* Every server handles both **reads and writes**.
* Every server acts as **master (write)** and **slave (read & replicate)**.
* All servers are **homogeneous** and communicate via **Gossip Protocol**.

---

## ğŸ§© Server vs Shard

* In **Master-Slave**, we have heterogeneous servers:

  * Load balancer, orchestrator, master, slave, reserved slave, etc.
* In **Multi-Master**, all servers are the same:

  * Each server handles LB, orchestration, reads, and writes.
  * Sharding & replication are merged.

**Implication:**

* Shards exist conceptually, but every server contains **multiple partitions**.
* Each server can contain **some primary partitions and some replica partitions**.

---

## Ring Placement

* Servers (not shards) are placed on the **consistent hashing ring**.
* No concept of a separate â€œshard crashâ€: if a server crashes, consistent hashing redirects traffic automatically.

---

## Data Replication & Partitioning

* Each piece of data is **written in multiple places**:

  * Some writes are **synchronous** (to meet consistency requirements).
  * Others may be **asynchronous**.
* Uses **tunable consistency** (parameters X, R, W).

**Example:**

* N = 4 Servers: `{A, B, C, D}`
* K = 5 virtual spots per server on the ring
* Replication factor X = 3
* Users `{u1 .. u20}`

**Partitions per server:**

| Server | Primary                | Replica 2              | Replica 3              |
| ------ | ---------------------- | ---------------------- | ---------------------- |
| A      | u1, u7, u11, u14, u16  | u6, u10, u13, u15, u20 | u19, u5, u9, u12, u14  |
| B      | u5, u8, u10, u13, u19  | u4, u7, u9, u12, u18   | u3, u6, u8, u11, u17   |
| C      | u3, u12, u15, u17, u20 | u2, u11, u14, u16, u19 | u1, u10, u13, u15, u18 |
| D      | u2, u4, u6, u9, u18    | u1, u3, u5, u8, u17    | u20, u2, u4, u7, u16   |

> Each userâ€™s data is replicated across **3 servers**.
> **Important:** Do **not** store multiple replicas of the same data on the same server â€” defeats replication purpose.

---

## ğŸ”„ Load Balancing & Routing

* **Master-Slave:** All requests go to LB first, then routed to master/slave.
* **Multi-Master:** Any server can receive requests.

  * Server uses **consistent hashing** to locate which servers contain the data.
  * Forward requests as needed.
  * Higher availability, no central LB required.

**Homogeneous Servers:**

* Each server acts as:

  * LB
  * Orchestrator
  * Master
  * Slave

**Communication:**

* Continuous peer-to-peer via **Gossip Protocol**
* Ensures metadata consistency and fault tolerance.

---

## ğŸ”¹ Key Advantages of Multi-Master

1. **No single point of failure** (no master dependency).
2. **Higher availability**: any server can handle requests.
3. **Simplified architecture**: homogeneous servers.
4. **Elastic scaling**: new servers join the ring and sync via gossip.
5. **Tunable consistency**: control trade-off between latency & data durability.

---

### ğŸ’¡ Visualization Idea

```
Consistent Hash Ring:

       A
      / \
     D---B
      \ /
       C

Server partitions (example, replication factor = 3):

u1 â†’ Primary: A | Replica: C, D
u2 â†’ Primary: D | Replica: C, B
u3 â†’ Primary: C | Replica: B, D
...
```

* Each server contains **primary and replica partitions**.
* Requests hitting any server are routed internally to the correct replicas.

---
