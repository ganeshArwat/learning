# ğŸ“± **Study Guide: Messaging Apps (System Design)**

---

## ğŸ§© **Problem Statement**

### ğŸ§  Reason by Analogy

Understand messaging apps by comparing them to existing products and their use cases.

| Category                   | Examples                                          | Characteristics                                                   |
| -------------------------- | ------------------------------------------------- | ----------------------------------------------------------------- |
| **Mobile-first Chat**      | WhatsApp, Telegram, Signal, Hike, iMessage        | - Realtime, low-latency chat<br>- Built as SMS replacement        |
| **Enterprise Chat**        | Slack, MS Teams, Flock                            | - B2B focus<br>- Group chat for workplace collaboration           |
| **Social Media Messaging** | Facebook Messenger, Instagram DMs, Orkut Messages | - Focus on connecting social users<br>- Primarily 1-to-1 messages |

---

## âš™ï¸ **Functional Requirements (FR)**

### ğŸ‘¥ Actors

* **User**: The primary participant in all messaging operations.

---

### ğŸš€ **MVP Features**

#### 1. Send Message (1-to-1)

```plaintext
sendMessage(sender_id: uuid, recipient_id: uuid, message: json) â†’ ack/failure
```

* Allows one user to send a message to another.

#### 2. Receive and View Messages

```plaintext
getMessages(user_id: uuid, conversation_id: uuid, pagination_offset: int, pagination_limit: int) â†’ list of messages
```

* Users can fetch past messages in a conversation (most recent first).
* Users can only access messages from conversations they are part of.

#### 3. View Conversations

```plaintext
getConversations(user_id: uuid, pagination_offset: int, pagination_limit: int) â†’ list of conversations
```

* Lists all conversations the user participates in (most recent first).

#### 4. Group Messaging

```plaintext
sendMessageGroup(sender_id: uuid, group_id: uuid, message: json) â†’ ack/failure
getMessagesGroup(user_id: uuid, group_id: uuid, pagination_offset: int, pagination_limit: int) â†’ list of messages
```

* Enables sending and viewing messages in groups.
* Groups can scale to **100,000+ users**.

---

## ğŸŒ± **Future Scope Features**

| Feature                     | Description                                                                                      |
| --------------------------- | ------------------------------------------------------------------------------------------------ |
| **Message Status Tracking** | Sent â†’ Received by server<br>Delivered â†’ Delivered to recipient<br>Read â†’ Viewed by recipient    |
| **Edit/Delete Messages**    | Modify or remove sent messages (complex to implement reliably)                                   |
| **Notifications**           | In-App + Push notifications                                                                      |
| **Multi-Device Support**    | Sync across devices using **CRDTs** (Conflict-free Replicated Data Types) â€” very complex feature |

---

## ğŸš« **Non-MVP / Out-of-Scope Features**

| Category                  | Examples                                               |
| ------------------------- | ------------------------------------------------------ |
| **Rich Messages**         | Emojis, Images, Videos, GIFs                           |
| **Profile Management**    | User bios, avatars, settings                           |
| **End-to-End Encryption** | Security-focused feature, hard to integrate early      |
| **Broadcast Messaging**   | Send same message to multiple users (batch processing) |


---

# âš™ï¸ **Non-Functional Requirements (NFR)**

---

## ğŸ” **1. Message Order**

* The **order of messages** in a conversation is **critical**.
* **Meaning can change** if messages are displayed out of order.
  Example:

  > â€œIâ€™m breaking up with you.â€ â†’ â€œJust kidding ğŸ˜‚â€
  > (If reversed, the meaning completely changes!)

âœ… **Goal:** Ensure messages are always displayed in **the exact sequence they were sent**.

---

## ğŸ”‚ **2. Idempotency**

* Messaging apps must handle **network (n/w) failures** gracefully.
* Clients often **auto-retry** sending messages when acknowledgments (ACKs) arenâ€™t received.

### âš™ï¸ **Auto-Retry Algorithm**

1. Client sends a request to the server.
2. Server processes the request successfully.
3. Server sends an **ACK** (acknowledgment).
4. Client receives ACK â†’ request considered complete.
5. If ACK **isnâ€™t received**, the client **retries** automatically.

---

### âš ï¸ **Problem Scenario**

If:

* The original request **reaches the server**
* Server **stores the message** and **sends an ACK**
* But the **ACK is lost** (network failure)

Then:

* The **client thinks** the message failed.
* It **retries** the request.
* Server **stores the same message again** â†’ **duplicate messages**.

---

### ğŸ’¡ **Solution: Idempotency Handling**

* Use **unique message IDs (UUIDs)** per request.
* Before storing a message, the server should:

  * **Check** if that message ID already exists.
  * If yes â†’ **ignore the duplicate**.
  * If no â†’ **store and acknowledge**.

âœ… **Result:** Prevents accidental duplicates caused by automatic retries.

---

### ğŸ˜… **Exception: Intentional Re-sends**

* If a **user intentionally** sends the same message multiple times (e.g., â€œHiâ€ spam),
  the system **should allow it**.
* Only **automatic retry duplicates** should be filtered out â€” not **user-initiated repeats**.

---
# âš–ï¸ **Consistency vs Availability**

---

## ğŸ§  **1. Consistency**

### ğŸ§© What Does *Eventual Consistency* (Stale Reads) Mean?

In distributed systems, **eventual consistency** means that all nodes will *eventually* reflect the same data â€” but **temporary stale reads** may occur.

In a messaging app, this can cause serious confusion and poor user experience.

---

### ğŸ’¬ **Examples of Stale Reads**

#### ğŸ§â€â™‚ï¸ **Sender Side Example**

* Shaurya sends a message to Harpal at **t = 0**.
* The **server ACKs** (acknowledges) the message.
* Shaurya reloads his app at **t = 1**, but **cannot see** the message he just sent.
* He thinks the message was **deleted or lost**, but in reality:

  * The message **exists**.
  * It just hasnâ€™t **replicated** to all databases yet.
  * After some delay, heâ€™ll **eventually see it**.

âœ… **Solution:**
Use **Read-Your-Write Consistency**, ensuring that a user can **immediately read their own writes**.

---

#### ğŸ‘¥ **Receiver Side Example**

* Shaurya sends a message to Harpal at **t = 0**.
* Server **ACKs** the message.
* At **t = 1**, Harpal opens the chat â€” but **doesnâ€™t see** the message yet.
* Shaurya insists he sent it, but Harpalâ€™s app still hasnâ€™t updated.
* Again, the message is **not lost**, only **delayed** due to eventual consistency.

âš ï¸ This delay can break the illusion of real-time chat â€” unacceptable in communication apps.

---

### ğŸš« **Conclusion**

> Messaging apps **cannot afford** stale reads.
> Real-time communication demands **strong consistency** to ensure both sender and receiver see the same message state instantly.

---

## â±ï¸ **2. Consistency vs Latency**

### âš”ï¸ **The Trade-Off**

According to the **PACELC Theorem**:

> *If thereâ€™s a network Partition (P), you must choose between Availability (A) and Consistency (C).
> Else (E), you must choose between Latency (L) and Consistency (C).*

| Goal                                | Challenge                                           |
| ----------------------------------- | --------------------------------------------------- |
| **Strong Consistency**              | Increases latency â€” messages take longer to confirm |
| **Low Latency & High Availability** | Can cause stale or out-of-order reads               |

---

### ğŸ’¡ **Practical Reality**

* Itâ€™s **impossible** to achieve **high consistency**, **high availability**, and **low latency** *simultaneously*.
* Butâ€¦ we can **create the illusion of consistency** through design techniques such as:

  * Caching with background sync
  * Message queues with delivery guarantees
  * Local echo (showing sent message instantly)

âœ… **Goal:**
Sacrifice a *tiny bit* of real consistency, but make the system **appear perfectly consistent** to users **99.999% of the time**.

---
# ğŸ“Š **Scale Estimation**

---

## ğŸŒ Assumptions

* Total Users (Web/Planet Scale): **2 billion**
* Daily Active Users (DAU): **80% of total = 1.6 billion â‰ˆ 2 billion**

---

## 1ï¸âƒ£ Amount of Load

**Average number of messages per active user per day:** 10â€“20 messages

**Total number of messages per day:**
= 20 messages Ã— 2 billion active users
= **40 billion messages per day**

**Average messages per second:**
= 40 billion / 100,000 seconds
= 4 Ã— 10âµ messages/second
= **400,000 messages per second**

**API Load (Average):**

* sendMessage(...): 400,000/sec
* getMessages(...): 2Ã— sendMessage = 800,000/sec
* getConversations(...): only when user reopens the app

So, the system handles roughly **400K writes/sec** and **800K reads/sec** on average.

---

## 2ï¸âƒ£ Peak Load

* During global events like New Year or pandemic spikes, assume **5Ã— average load**.
* Peak = 5 Ã— 400,000 = **2 million messages per second**

This shows the system must **scale horizontally** to manage extreme peaks.

---

## 3ï¸âƒ£ Amount of Data (Over 20 Years)

**Message object:**

```
{
  sender_id: uuid (16B)
  receiver_id/group_id/conversation_id: uuid (16B)
  message_id: uuid (16B)
  text: string (200B avg)
  when: timestamp (8B)
  where: geolocation (16B)
  delivery_status: enum (1B)
  attachment_url: string (200B avg)
}
```

**Average size:** ~500 bytes/message

**Data per day:**
500 bytes Ã— 40 billion messages = **20 TB/day**

**Data over 20 years:**
20 TB/day Ã— 365 days Ã— 20 years â‰ˆ **200 Petabytes (PB)**

A single server can store up to ~2 PB, far below the total requirement.
Even with enough capacity, a single server canâ€™t handle **2 million messages/sec**.

âœ… **Conclusion:** We definitely need **sharding** â€” distributing both storage and load across multiple servers.

---

## 4ï¸âƒ£ Read-heavy or Write-heavy?

* Writes (sendMessage): 400K/sec avg â†’ 2M/sec peak
* Reads (getMessages): 800K/sec avg â†’ 4M/sec peak

Both reads and writes are significant; **neither dominates** the workload.
This system is **both read- and write-heavy**.

We canâ€™t reduce writes (each message must be sent).
To optimize, we reduce **database reads** by caching.

âœ… **Final Insight:**
Weâ€™ll use **lots of caching** to absorb reads and keep the database optimized for high write throughput.

---

# ğŸ—ï¸ **System Design**

---

## ğŸ” **Idempotency & Message Order**

---

### ğŸ†” **Message Identification**

* Every message must have a **unique `message_id`**, generated on the **client side** (not server side).
* This ensures that retries or resends can be tracked precisely and deduplicated safely.

---

## ğŸ”„ **Idempotency**

* When the backend receives a `sendMessage()` API call:

  * It checks if the `message_id` already exists in the database.
  * **If not present:** it stores the message normally.
  * **If already present:** it **ignores** the request but still sends a **success response** to the client.

âœ… This behavior prevents **duplicate messages** caused by automatic retries from the frontend when acknowledgments (ACKs) are delayed or lost.

ğŸ’¡ **Goal:**
Avoid duplicate message storage while ensuring client-side retries stop after the first successful send.

---

## ğŸ§© **Message Order**

### 1ï¸âƒ£ **Why Simple Sorting Fails**

* Sorting messages based on `created_time` or timestamps **does not work** reliably.
* The recipient doesnâ€™t even know that a new message exists until itâ€™s delivered.
* By the time a delayed (older) message arrives, **newer messages** may already have been shown in the chat.
* The backend (message queue, SQS, or buffer) **cannot fix this** â€” it only processes messages once they arrive.

  * By then, the order issue has already occurred.

---

### 2ï¸âƒ£ **Why Backend Solutions Donâ€™t Help**

* Even if you use backend buffering or reordering logic:

  * The server still doesnâ€™t know about a message until it arrives.
  * Once later messages are already delivered to clients, itâ€™s **too late** to reinsert an older one in sequence.

---

### ğŸ’¡ **Solution: Create a Message Chain**

* Every new message includes a reference to its **`previous_message_id`**.
* On the recipientâ€™s side:

  * The app **wonâ€™t display** a message until its **previous message** has also arrived and been displayed.
  * If messages arrive **out of order**, the app shows a **â€œwaiting for messagesâ€¦â€** placeholder.
  * Once all previous messages are received, the full chat appears in the correct order.

âœ… **Result:**
This client-side chaining ensures **correct message ordering** even with **network delays** or **out-of-order delivery**.

---

## ğŸ§© Sharding

### ğŸ”‘ Sharding Key: `user_id`

**Meaning:**
All data (messages sent and received) of a particular user is stored within the same shard.

#### ğŸ—¨ï¸ 1-to-1 Messages

* **`getMessages(user_id, conversation_id, â€¦)`** â†’ Query only the userâ€™s shard.
* **`sendMessage(sender_id, receiver_id, â€¦)`** â†’ Write to both sender and receiver shards (data replication).
* **`recentConversations(user_id)`** â†’ Query only the userâ€™s shard.

#### ğŸ‘¥ Group Messages

* **`getMessages(user_id, group_id, â€¦)`**

  1. **Store in Sender Only:**

     * Each senderâ€™s messages stay in their own shard.
     * Leads to **fan-out reads** (messages spread across shards).
  2. **Store in All Participants:**

     * Fast reads (can read from any shard).
     * But causes **fan-out writes** (message replicated across all shards).

* **`sendMessage(sender_id, group_id, â€¦)`**

  1. **Store in Sender Only:**

     * Stored only in senderâ€™s shard.
  2. **Store in All Participantsâ€™ Shards:**

     * Causes fan-out writes.

* **`recentConversations(â€¦)`**

  * Sharding by `user_id` **does not work well for group chats**.
  * Either reads or writes become fan-out operations.

---

### ğŸ”‘ Sharding Key: `conversation_id`

**Meaning:**
All messages within a conversation (1-to-1 or group) go to the same shard.

* For **groups**, `conversation_id = group_id`
* For **1-1 chats**, `conversation_id` = unique ID for the user pair

#### ğŸ—¨ï¸ 1-to-1 Messages

* **`getMessages(user_id, conversation_id, â€¦)`** â†’ Query the conversationâ€™s shard.
* **`sendMessage(sender_id, receiver_id, â€¦)`** â†’ Write to the conversationâ€™s shard.

  * Conversation ID = pair (`sender_id`, `receiver_id`)
* **`recentConversations(user_id)`**

  * Requires **fan-out** across shards â†’ inefficient.
  * âœ… **Solution:** Maintain a **separate `recentConversations` DB** that tracks last message timestamps for all conversations.
  * Every `sendMessage` updates this DB.
  * The name â€œrecentConversations DBâ€ reflects its usage, not its data scope.

#### ğŸ‘¥ Group Messages

* **`getMessages(user_id, group_id, â€¦)`** â†’ Query the groupâ€™s shard.
* **`sendMessage(sender_id, group_id, â€¦)`** â†’ Write to the groupâ€™s shard.
* **`recentConversations(user_id)`**

  * Updating this for large groups (e.g., 100K members) leads to **massive fan-out**.
  * âŒ Avoid this API for groups.
  * âœ… Instead, the **frontend** maintains a **local cache of recent conversations**, updated via **notifications**.

---

### âš–ï¸ Choosing the Right Sharding Key

| Use Case                          | Best Sharding Key | Reason                                                         |
| --------------------------------- | ----------------- | -------------------------------------------------------------- |
| **Facebook Messenger / WhatsApp** | `user_id`         | Groups are small; user-centric sharding works well.            |
| **Slack / Microsoft Teams**       | `conversation_id` | Groups are large; conversation-centric sharding scales better. |

---

## âš–ï¸ Consistency

### ğŸ§© PACELC: Consistency vs Availability/Latency

**Problem:**
We want strong consistency across shards, but maintaining it atomically hurts latency and availability.

---

### Option 1: Two-Phase Commit (2PC)

* Write atomically to both shards.
* âŒ **Extremely slow:** low throughput, high latency.
* âŒ **Low availability:** locks resources during commit.

---

### ğŸ’¡ Alternative: Illusion of Consistency

Instead of atomic writes, we **write one shard at a time** to balance speed and availability.

---

### âœ‰ï¸ Write to **Senderâ€™s Shard First**

1. **Write to Senderâ€™s shard**

   * **Failure (low latency):**
     â†’ Immediately return failure â†’ client retries.
   * **Success:**

     * Can we return success yet?
       âŒ No, because the message isnâ€™t in the receiverâ€™s shard yet.

       * Sender believes the message was sent.
       * Receiver doesnâ€™t see it â†’ **inconsistent state.**

         * Sender: â€œI sent it.â€
         * Receiver: â€œI didnâ€™t get it.â€

2. **Then write to Receiverâ€™s shard**

   * **Success (low latency):**
     â†’ Return success (both shards updated).
   * **Failure (high latency):**

     * Data inconsistent â†’ message in senderâ€™s shard but missing in receiverâ€™s.
     * Must **retry** or **rollback.**

       * Retry: sender waits = **high latency**.
       * Rollback: locks senderâ€™s shard until rollback finishes â†’ **slow reads.**

â¡ï¸ **Verdict:** Writing sender first causes high latency and temporary inconsistency.

---

### âœ‰ï¸ Write to **Receiverâ€™s Shard First**

1. **Write to Receiverâ€™s shard**

   * **Failure (low latency):**
     â†’ Return failure immediately â†’ sender retries.
   * **Success:**

     * Recipient can already see the message!
     * âœ… Return **success immediately** (even before writing senderâ€™s shard).
     * Very **low latency**.

2. **(Async) Write to Senderâ€™s shard**

   * **Success:** done.
   * **Failure:** retry silently; sender already got ACK, so no waiting.

âš ï¸ **Edge Case:**
If writing to receiver succeeds but writing to sender fails,
â†’ When sender reloads, they donâ€™t see their sent message â†’ confusion.

âœ… **Fix:** Frontend cache in the client app.

* Cache stores all sent messages that were acknowledged.
* Even if `getMessages` API doesnâ€™t show it yet, the app displays it from cache.

---

### ğŸª„ Result

By writing to the **receiverâ€™s shard first** and using a **frontend cache** for the sender,
we achieve the **illusion of immediate consistency** â€”
high availability and low latency without full atomic writes.

---

### âš™ï¸ Consistency vs Availability (Partition Scenario)

**Example:**
Harini â†’ â€œHiâ€ â†’ Aditi

* App Server 1 receives Hariniâ€™s message.
* Network partition: Server can reach **Aditiâ€™s shard**, but not **Hariniâ€™s shard**.

**Process:**

1. Server writes to Aditiâ€™s shard â†’ âœ… success.
2. Returns success to Harini.
3. Queues a background task to write the message to Hariniâ€™s shard later (async).

âœ… **Outcome:**

* System remains available:

  * Aditi can see the message (recipient write succeeded).
  * Harini sees it too (via frontend cache).
* Gives illusion of **immediate consistency** even during a partition.

âŒ If write to Aditiâ€™s shard fails â†’ return error â†’ Harini retries.

---

### âš¡ Consistency vs Latency

* No atomic writes, no waiting for rollback/retry.
* Return success as soon as recipientâ€™s shard write succeeds.
* âœ… **Very low latency** and practically consistent for users.

---

## ğŸ—„ï¸ Choosing the Right Database

---

### âš¡ Workload Overview

* **Writes (`sendMessage`)**: 400,000/sec avg, 2 million/sec peak
* **Reads (`getMessages`)**: 800,000/sec avg, 4 million/sec peak
* **Observation:** Both **read- and write-heavy** â€” no single database excels at both natively.

---

### ğŸ”§ Optimizing Reads & Writes

**Can we optimize writes?**

* **Batching:** reduces write calls but causes stale reads â†’ âŒ not acceptable
* **Sampling:** reduces write volume but risks data loss â†’ âŒ not acceptable

**Can we optimize reads?**

* âœ… Absorb most reads via **cache layer**
* With reads handled by cache, the **database can be optimized for writes**

---

### ğŸ† Requirements for the Database

1. High write throughput (~400K/sec avg)
2. No joins required (queries are scoped to a conversation)
3. No need for search (can add separate search service later)
4. Supports paginated reads (by timestamp)
5. No need for transactions
6. Disk persistence required

---

### ğŸ”¹ Database Options

* **SQL**

  * Strengths: ACID transactions, normalization, joins
  * Weaknesses: Low throughput, difficult horizontal scaling

* **Key-Value**

  * Strengths: High throughput, simple
  * Weaknesses: No joins, no search, no native pagination â†’ âŒ we need pagination

* **Document**

  * Strengths: Schemaless, search, local indexes
  * Weaknesses: No joins, limited pagination â†’ âŒ we need pagination, search not required

* **Column-Family / Wide Column DB**

  * Strengths: Fast writes, time-based pagination, fast aggregates
  * Weaknesses: No joins, no search â†’ âœ… matches our requirements perfectly

---

### âœ… **Ideal Choice**

* **Wide Column Database** (HBase, Cassandra, ScyllaDB)
* Optimized for **write-heavy workloads**, supports **time-based pagination**, and scales horizontally.

---

## ğŸ—‚ï¸ Cache

---

### ğŸ”¹ Local vs Global Cache

* **Single Global Cache:** âŒ Not feasible

  * With billions of users and high throughput, one server cannot hold all cached data.
  * Would create wasted app servers (just forwarding requests to cache servers).

* **Distributed Global Cache:** âœ… Feasible, but:

  * App servers mostly forward requests â†’ underutilized.
  * Requires double infrastructure (app servers + cache servers).

* **Local Cache (preferred):** âœ…

  * Each app server doubles as cache.
  * Reduces network overhead and infrastructure cost.
  * Handles most read requests efficiently (`getMessages(user_id, conversation_id, pagination)`).

---

### ğŸ”¹ Invalidation

* Goal: **Immediate consistency** without high latency.
* Separate cache server requires 2-phase commit â†’ high latency.
* Local cache allows **write-through invalidation**:

  * Writing to the cache always succeeds.
  * Keeps cache and database consistent without 2PC.

---

### ğŸ”¹ Eviction

* Use **LRU (Least Recently Used)** strategy to evict old messages.

---

### ğŸ”¹ Routing

* App servers are **stateful**.
* If an app server holds a userâ€™s messages in its cache:

  * All requests for that user should be routed to the same app server.
  * **Consistent hashing** ensures this.

**Example Flow:**

1. **getMessages(user_id, conversation_id)**

   * Routed to the shard + app-server holding the local cache.
2. **sendMessage(sender_id, recipient_id)**

   * Routed to the **recipientâ€™s shard + cache first**.
   * Data is then asynchronously replicated to the **senderâ€™s shard + cache** via a message queue.

âœ… **Benefits:**

* Low latency reads
* Immediate consistency
* Efficient replication

---
