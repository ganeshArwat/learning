# âš–ï¸ **Load Balancing**

Load balancing determines **which server should handle a particular request**.
It ensures even distribution of traffic, optimal resource utilization, and high availability.

In **data-aware systems** (like sharded databases, distributed caches, etc.), load balancing is tightly coupled with **data distribution** â€” because if a request goes to the wrong server, the data wonâ€™t be found.

> Example:
> If Sanjanaâ€™s user profile is stored on **Server B**, but the load balancer routes her request to **Server A**, the request fails (or returns â€œUser not foundâ€).

---

## ğŸ§© **1ï¸âƒ£ Data Sharding**

**Sharding** is the process of **splitting large datasets into smaller, more manageable pieces (shards)** and distributing them across multiple servers.
It improves **scalability**, **parallelism**, and **fault isolation**.

---

### **Types of Sharding**

#### 1. Vertical Partitioning (Normalization)

* Data is divided by **columns (attributes)**.
* Each table or microservice handles a subset of attributes.

**Example:**

```
User(id, name, age, address)
â†’ UserBasic(id, name, age)
â†’ UserAddress(id, address)
```

âœ… Efficient queries for specific attributes
âŒ Requires joins or multiple calls to rebuild full entity

---

#### 2. Horizontal Partitioning (Row-wise)

* Data is divided by **rows (records)**.
* Each shard stores a **subset of users or objects**.

**Example:**

```
Server1 â†’ user_id 1â€“1000  
Server2 â†’ user_id 1001â€“2000
```

âœ… Most common approach in large systems
âœ… Each shard is independent
âŒ Choosing the right sharding key is critical (to avoid hotspots)

---

#### 3. Vertical Partitioning Across Servers

* Different **columns** live on different **physical servers**.
* Used when tables are very wide or have heavy column access patterns.

---

#### 4. Horizontal Partitioning Across Servers

* Each server holds **different rows** of a single logical table.
* Typically used with **routing algorithms** (hashing, consistent hashing, etc.) for distributing queries.

---

## ğŸ”‘ **2ï¸âƒ£ Sharding Key**

A **sharding key** is the attribute that decides **where a record lives**.

**Example:**

* `user_id` for user-related data
* `region_id` for geo-distributed systems
* `order_id` for e-commerce orders

### **Good Sharding Key Characteristics**

* **Uniformly distributed** (avoids hotspots)
* **Immutable** (should not change over time)
* **Present in most queries** (to simplify routing)

---

## ğŸ§­ **3ï¸âƒ£ Sharding & Routing**

Routing logic must **exactly match** the sharding logic.
Otherwise, a query might hit the wrong shard.

Example:

* Sharding Logic: `hash(user_id) % N`
* Routing Logic: must use the same `hash(user_id) % N`

Mismatch â†’ data inconsistencies or failed lookups.

---

## âš™ï¸ **Routing Algorithm: Key Characteristics**

A good routing algorithm must be:

| Property                  | Description                                                |
| ------------------------- | ---------------------------------------------------------- |
| **Fast**                  | Must quickly decide which node to use                      |
| **Equal Distribution**    | Should spread requests evenly                              |
| **Elastic**               | Adding/removing servers should not cause major reshuffling |
| **Minimal Data Movement** | Data migration should be small                             |
| **Deterministic**         | Same input â†’ same server without coordination              |

---

# ğŸ” **Round Robin Load Balancing**

Round Robin is the **simplest** and most widely used load balancing strategy.

---

### **1ï¸âƒ£ How It Works**

* Requests are distributed **sequentially** across all servers.
* Each new request goes to the **next server in the list**, looping back after the last.

**Flow:**
`A â†’ B â†’ C â†’ A â†’ B â†’ C â†’ ...`

---

### **2ï¸âƒ£ Example**

| Request | Server |
| ------- | ------ |
| R1      | A      |
| R2      | B      |
| R3      | C      |
| R4      | A      |
| R5      | B      |
| R6      | C      |

---

### **3ï¸âƒ£ Variants**

* **Simple Round Robin:** Equal turn for all servers.
* **Weighted Round Robin:** Assigns different weights based on server capacity.
  Example: A(2x), B(1x) â†’ sequence: A, A, B, A, A, Bâ€¦

---

### **4ï¸âƒ£ Adding & Removing Servers**

* When adding/removing servers, rotation order changes.
* Does **not** preserve session affinity.
* Existing clients may lose session continuity.

---

### **5ï¸âƒ£ Pros**

âœ… Simple, stateless, no metadata needed
âœ… Good for **homogeneous** servers
âœ… Works well for **stateless applications**

---

### **6ï¸âƒ£ Cons**

âŒ Ignores server load or response time
âŒ No data-awareness
âŒ Not ideal for **session persistence** or **stateful workloads**

---

### **7ï¸âƒ£ When to Use**

* All servers have **similar capacity**
* Applications are **stateless**
* Need quick, simple balancing
* Example: REST APIs, static content servers

---

# ğŸª£ **Bucketing (Modulo) Load Balancing**

Bucketing (or Modulo-based hashing) uses a **hash function** to deterministically map each user/request to a specific server.

---

### **1ï¸âƒ£ How It Works**

Each request is hashed â†’ hash result is divided by total servers â†’ server index chosen.

```
server_index = hash(key) % N
```

* All requests with same key go to same server.
* Predictable and data-aware.

---

### **2ï¸âƒ£ Example**

| Server | Range |
| ------ | ----- |
| A      | 0â€“3   |
| B      | 4â€“6   |
| C      | 7â€“9   |

User `u1` â†’ hash(u1)=2 â†’ A
User `u2` â†’ hash(u2)=8 â†’ C

---

### **3ï¸âƒ£ Adding/Removing Servers**

* Adding/removing servers changes `N` â†’ **all hashes shift!**
* Results in **massive remapping** of keys.

---

### **4ï¸âƒ£ Pros**

âœ… Fast and deterministic
âœ… Simple to implement
âœ… Data-aware (same key â†’ same server)

---

### **5ï¸âƒ£ Cons**

âŒ High data movement when servers change
âŒ Poor scalability
âŒ Requires rebalancing entire dataset

---

### **6ï¸âƒ£ When to Use**

* Cluster size rarely changes
* Small or fixed number of servers
* Systems where rehashing cost is acceptable

---

# ğŸ—ºï¸ **Mapping Table Load Balancing**

A **Mapping Table** explicitly stores which **key/user** belongs to which **server**.

---

### **1ï¸âƒ£ How It Works**

A lookup table defines:

```
User/Key â†’ Server
```

Every request performs a quick lookup.

---

### **2ï¸âƒ£ Example**

| User | Server |
| ---- | ------ |
| u1   | A      |
| u2   | B      |
| u3   | C      |
| u4   | B      |
| u5   | A      |

---

### **3ï¸âƒ£ Advantages**

âœ… Complete control over mapping
âœ… No hash dependency
âœ… Perfect for **sticky sessions** and **custom routing**

---

### **4ï¸âƒ£ Disadvantages**

âŒ High memory usage for large datasets
âŒ Operational overhead to maintain table
âŒ Single point of failure unless replicated

---

### **5ï¸âƒ£ When to Use**

* Stateful workloads (sessions, chat systems, custom shards)
* Fixed or small number of users
* Custom or manual routing policies

---

# ğŸŒ€ **Consistent Hashing**

Consistent hashing solves the **scalability problem** of modulo hashing by minimizing key movement when servers are added or removed.

---

### **1ï¸âƒ£ Hash Function**

Maps keys and servers into a numeric space (e.g., 0 â†’ 2Â³Â² âˆ’ 1).

**Examples:**
`hash("ServerA")`, `hash("User123")` using MD5, SHA-1, MurmurHash.

---

### **2ï¸âƒ£ Hash Ring**

* Imagine the numeric space as a **circular ring**.
* Both **servers** and **keys** are hashed and placed on this ring.
* A key belongs to the **first server clockwise** from its hash position.

---

### **3ï¸âƒ£ Example**

| Entity   | Hash Position |
| -------- | ------------- |
| Server A | 10            |
| Server B | 50            |
| Server C | 80            |

Keys:

* Key k1 (15) â†’ Server B
* Key k2 (75) â†’ Server C
* Key k3 (5) â†’ Server A

---

### **4ï¸âƒ£ Virtual Nodes**

Each physical server can have multiple **virtual nodes (vnodes)** to achieve smoother load distribution.

Example:
Server A â†’ hash(A1), hash(A2), hash(A3) â€¦
Spreads load evenly across ring.

---

### **5ï¸âƒ£ Adding/Removing Servers**

**Adding:**

* Insert new server on ring.
* Only keys between the new server and its predecessor move.

**Removing:**

* Keys on removed server move to its successor.

âœ… Minimal reshuffling
âœ… Great for dynamic clusters

---

### **6ï¸âƒ£ Pros**

âœ… Scales dynamically with minimal key movement
âœ… Data-aware and deterministic
âœ… Supports virtual nodes for fairness
âœ… Perfect for caches, databases, and message systems

---

### **7ï¸âƒ£ Cons**

âŒ Slightly higher implementation complexity
âŒ Needs a good hash function
âŒ Slight imbalance in small clusters without vnodes

---

### **8ï¸âƒ£ Used In**

* **Cassandra**, **DynamoDB**, **Redis Cluster**, **Kafka**, **Memcached**, **Elasticsearch**

---

# ğŸ§® **Comparison Table**

| Algorithm              | Data-Aware | Key Movement on Node Change | Complexity | Ideal For                          |
| ---------------------- | ---------- | --------------------------- | ---------- | ---------------------------------- |
| **Round Robin**        | âŒ          | N/A                         | Simple     | Stateless web servers              |
| **Modulo / Bucketing** | âœ…          | High (reshuffle all)        | Simple     | Small static clusters              |
| **Mapping Table**      | âœ…          | Manual (low)                | Medium     | Sticky sessions, custom routing    |
| **Consistent Hashing** | âœ…          | Minimal                     | Medium     | Large, dynamic distributed systems |

---

# âš™ï¸ **Stateless Servers**

A **stateless server** doesnâ€™t store client-specific state between requests.
Each request is self-contained â€” it carries all data required for processing.

---

### **1ï¸âƒ£ Characteristics**

* No session data in memory
* Requests contain authentication or context
* Servers can be freely added/removed
* Perfect for autoscaling environments

---

### **2ï¸âƒ£ Examples**

* REST APIs (each request has JWT token)
* Microservices that query state from DB/cache instead of local memory

---

### **3ï¸âƒ£ Pros**

âœ… Easy horizontal scaling
âœ… No session replication needed
âœ… High availability and fault tolerance
âœ… Simplifies load balancing (any node can serve any request)

---

### **4ï¸âƒ£ Cons**

âŒ State must live externally (DB, cache, session store)
âŒ Slightly more network overhead (token verification, extra lookups)

---

### **5ï¸âƒ£ When to Use**

* High-traffic systems (web APIs, cloud apps)
* Stateless microservices
* Auto-scaled server fleets
* Event-driven or ephemeral environments (e.g., AWS Lambda)

---

# ğŸ§  **Final Summary**

| Concept                | Purpose                       | Best Use Case                   |
| ---------------------- | ----------------------------- | ------------------------------- |
| **Round Robin**        | Simple even distribution      | Stateless workloads             |
| **Bucketing / Modulo** | Deterministic mapping         | Fixed cluster                   |
| **Mapping Table**      | Explicit control              | Sticky sessions, small clusters |
| **Consistent Hashing** | Scalable, minimal remap       | Caches, DBs, dynamic clusters   |
| **Stateless Servers**  | Scalability & fault tolerance | APIs, microservices             |

---
